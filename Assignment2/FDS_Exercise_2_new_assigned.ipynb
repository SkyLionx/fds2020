{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSfK3TzzOeBK"
   },
   "source": [
    "# Fundamentals of Data Science\n",
    "Winter Semester 2020\n",
    "\n",
    "## Prof. Fabio Galasso, Alessandro Flaborea and Luca Franco\n",
    "<galasso@di.uniroma1.it>, <flaborea.1841969@studenti.uniroma1.it>, <franco.luca.17@gmail.com>\n",
    "\n",
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laIa2DFiOeBL"
   },
   "source": [
    "In Exercise 2, you will re-derive and implement logistic regression and optimize the parameters with Gradient Descent and with the Newton's method. Also, in this exercise you will re-derive and implement Gassian Discriminant Analysis.\n",
    "We will use the files logistic_x.txt and logistic_y.txt. The first contains the feature values $x^{(i)}_1$ and $x^{(i)}_2$ for the $i$-th data sample $x^{(i)}$. The second contains the ground truth label $y^{(i)}$ for each corresponding data sample.\n",
    "\n",
    "Note that, differently from what seen in lecture, $y^{(i)}\\in \\{-1,1\\}$.\n",
    "\n",
    "The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n",
    "Submit it by sending an email to galasso@di.uniroma1.it, flaborea.1841969@studenti.uniroma1.it and franco.luca.17@gmail.com by Wednesday November 25th, 23:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK9pUVWmOeBM"
   },
   "source": [
    "## Notation\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features\n",
    "\n",
    "Let's start by setting up our Python environment and importing the required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpM7R9w8OeBN"
   },
   "source": [
    "\n",
    "## [15 points] Question 1: Logistic Regression with Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96Z43pcuOeBN"
   },
   "source": [
    "### (a) [5 points] Equations for the log likelihood, its gradient, and the gradient ascent update rule.\n",
    "\n",
    "Write and simplify the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
    "\n",
    "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
    "\n",
    "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation. \n",
    "\n",
    "Question: in order to learn the optimal $\\theta$, do you need to minimize or to maximize $l(\\theta)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryllEcxDOeBO"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgkYGHGSOeBO"
   },
   "source": [
    "Your equations and answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEco3MQ1OeBP"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nCv4HmWOeBP"
   },
   "source": [
    "### (b) [10 points] Implementation of logistic regression with Gradient Ascent\n",
    "\n",
    "Code up the equations above to learn the logistic regression parameters. Consider as data the files logistic_x.txt and logistic_y.txt. The first contains the feature values $x^{(i)}_1$ and $x^{(i)}_2$ for the $i$-th data sample $x^{(i)}$. The second contains the ground truth label $y^{(i)}$ for each corresponding data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9qXilF-OeBQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp # imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm # allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt # sets up plotting under plt\n",
    "import pandas as pd # lets us handle data as dataframes\n",
    "\n",
    "# sets up pandas table display\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "import seaborn as sns # sets up styles and gives us more plotting options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBZrP2jQOeBU"
   },
   "source": [
    "Let's start by loading the data into two pandas DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "AIYevbmGOeBU",
    "outputId": "754a01ff-66bb-487d-99ac-bd7be41f4183"
   },
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(\"./data/logistic_x.txt\", sep=\"\\ +\", names=[\"x1\",\"x2\"], header=None, engine='python')\n",
    "df_y = pd.read_csv('./data/logistic_y.txt', sep='\\ +', names=[\"y\"], header=None, engine='python')\n",
    "df_y = df_y.astype(int)\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "yzWVpxZKOeBY",
    "outputId": "087f03ed-90c2-43ba-ecc0-4e1eede2698e"
   },
   "outputs": [],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvJngk29OeBb"
   },
   "source": [
    "Let's get the NumPy arrays from the dataset, and add a column of 1's to $X$ to take into account the zero intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2Jz97G4OeBb"
   },
   "outputs": [],
   "source": [
    "x = np.hstack([np.ones((df_x.shape[0], 1)), df_x[[\"x1\",\"x2\"]].values])\n",
    "y = df_y[\"y\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyWYlB43OeBe"
   },
   "source": [
    "Adjust y so that $y^{(i)}\\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5njaB8wOeBe"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl6TZcBYOeBf"
   },
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzS95OcYOeBf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh_JgsQoOeBi"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "_S8E0b9_OeBj",
    "outputId": "8c33615a-da97-48e0-8ca4-2f64b60b1093"
   },
   "outputs": [],
   "source": [
    "[x[:5,:],x[-5:,:]] # Plot the first and last 5 lines of x, now containing features x0 (constant=1), x1 and x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DZFFdmUqOeBm",
    "outputId": "e68613e6-4b26-4d1b-e22e-3f9b82e8b3cc"
   },
   "outputs": [],
   "source": [
    "[y[:5],y[-5:]] # Plot the first and last 5 lines of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe5LdnkmOeBo"
   },
   "source": [
    "Define the sigmoid function \"sigmoid\", the function to compute the gradient of the log likelihood  \"grad_l\" and the gradient ascent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK3e_-LmOeBp"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9D5RR6XOeBp"
   },
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seSHjGU7OeBq"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    pass # return the sigmoid of x\n",
    "    # return g\n",
    "\n",
    "def log_likelihood(theta,x,y):\n",
    "    pass # return the log likehood of theta according to data x and label y\n",
    "    # return log_l\n",
    "\n",
    "def grad_l(theta, x, y):\n",
    "    pass # return the gradient G of the log likelihood\n",
    "    # return G\n",
    "\n",
    "def gradient_ascent(theta,x,y,G,alpha=0.01,iterations=100):\n",
    "\n",
    "    m = len(y)\n",
    "    log_l_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,3))\n",
    "\n",
    "    pass # return the optimized theta parameters,\n",
    "        # as well as two lists containing the log likelihood's and values of theta at all iterations\n",
    "        \n",
    "    # return theta, log_l_history, theta_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne6TYR_GOeBu"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your grad_l implementation:\n",
    "grad_l applied to the theta_test (defined below) should provide a value for log_l_test close to the target_value (defined below); in other words the error_test should be 0, up to machine error precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = -0.4250958770469834\n",
    "theta_test=np.array([-2,1,2])\n",
    "\n",
    "log_l_test  = log_likelihood(theta_test,x,y)\n",
    "error_test=np.abs(log_l_test-target_value)\n",
    "\n",
    "print(\"{:f}\".format(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjpqRLekOeBv"
   },
   "source": [
    "Let's now apply the function gradient_ascent and print the final theta as well as theta_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "E9YW5A-aOeBv",
    "outputId": "2c0e0da0-f9e9-4bb7-bbc2-6d5978ac9059"
   },
   "outputs": [],
   "source": [
    "# Initialize theta0\n",
    "theta0 = np.zeros(x.shape[1])\n",
    "\n",
    "# Run Gradient Ascent method\n",
    "n_iter=1000\n",
    "theta_final, log_l_history, theta_history = gradient_ascent(theta0,x,y,grad_l,alpha=0.5,iterations=n_iter)\n",
    "print(theta_final)\n",
    "print(theta_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFv3R0V4OeBy"
   },
   "source": [
    "Let's plot the log likelihood over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "8e4XivQrOeBy",
    "outputId": "c3b202b0-bd07-4690-d310-673f04959bb4"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(num=2)\n",
    "\n",
    "ax.set_ylabel('l(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(len(log_l_history)),log_l_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0sfyGRyOeB1"
   },
   "source": [
    "Plot the data and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "KQClMhK9OeB2",
    "outputId": "73ca95f2-de91-419b-9188-f98fb9b5386f"
   },
   "outputs": [],
   "source": [
    "df_x_pl=df_x.copy()\n",
    "df_y_pl=df_y.copy()\n",
    "df_x_pl.insert(0, \"y\", df_y)\n",
    "df_x_pl[\"y\"] = pd.to_numeric(df_x_pl[\"y\"], downcast='signed')\n",
    "df_x_pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "o7UgpsNFOeB4",
    "outputId": "0c716ca2-ca3c-42fc-b731-c9e3992b27c1"
   },
   "outputs": [],
   "source": [
    "# Generate vector to plot decision boundary\n",
    "x1_vec = np.linspace(df_x_pl[\"x1\"].min(),df_x_pl[\"x1\"].max(),2);\n",
    "\n",
    "# Plot raw data\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"y\", data=df_x_pl);\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.plot(x1_vec,(-x1_vec*theta_final[1]-theta_final[0])/theta_final[2], color=\"red\");\n",
    "\n",
    "# Save the theta_final value for later comparisons\n",
    "theta_GA = theta_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VztviyAOeB7"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeTYW9SlOeB8"
   },
   "source": [
    "Write now your considerations. Discuss in particular:\n",
    "- what effects do the learning rate $\\alpha$ and the number of iterations have on the computed $\\theta$\n",
    "- may you still achieve the same value if you decrease the learning rate $\\alpha$ and increase the number of iterations n_iter up to an infinite number of them? Why?\n",
    "- may you still achieve the same value if you increase the learning rate $\\alpha$ substantially and decrease the number of iterations n_iter? Why?\n",
    "\n",
    "Substantiate your answer with experiments and plots of the $l(\\theta)$ and the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOZ8520WOeB9"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN7JwWH8OeB9"
   },
   "source": [
    "\n",
    "## [15 points] Question 2: Logistic Regression with the Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_lAFSxwOeB-"
   },
   "source": [
    "### (a) [5 points] Equations for the Hessian of the log likelihood\n",
    "\n",
    "Derive the equation for the Hessian matrix $H$ of the log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
    "\n",
    "Recall that: $H_{i,j} = \\frac{\\delta^2 l(\\theta)}{\\delta \\theta_i \\delta \\theta_j}$\n",
    "\n",
    "Also write the parameter update rule for the Newton's method, as a function of the Hessian and gradient of $l(\\theta)$.\n",
    "\n",
    "Question: in order to learn the optimal $\\theta$, do you need to minimize or to maximize $\\nabla_{\\theta} l$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7tXIk65OeB-"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUTclFjBOeB_"
   },
   "source": [
    "Your equations and the answer to the question here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2gKz7VDOeB_"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c-C9dZeOeCA"
   },
   "source": [
    "### (b) [10 points] Implementation of logistic regression with the Newton's method\n",
    "\n",
    "Code up the equations above to learn the logistic regression parameters. Consider as data the files logistic_x.txt and logistic_y.txt. The first contains the feature values  $x^{(i)}_1$ and $x^{(i)}_2$  for the  𝑖 -th data sample  $x^{(i)}$ . The second contains the ground truth label  $y^{(i)}$  for each corresponding data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9IGQc9_OeCA"
   },
   "source": [
    "Define the Hessian function hess_l and the Newton's method function \"newton\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKOYOByzOeCB"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yk_i8EhOeCB"
   },
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxH5xTi0OeCC"
   },
   "outputs": [],
   "source": [
    "def hess_l(theta, x, y):\n",
    "    pass # return the Hessian matrix hess\n",
    "    # return hess\n",
    "\n",
    "def newton(theta0, x, y, G, H, eps):\n",
    "    pass # return the optimized theta parameters,\n",
    "        # as well as two lists containing the log likelihood's and values of theta at all iterations\n",
    "    # return theta, theta_history, log_l_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6z0jueTOeCG"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your hess_l implementation: hess_l applied to the theta_test (defined below) should provide a value for hess_l_test close to the target_value (defined below); in other words the error_test should be 0, up to machine error precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = np.array([[-0.14357024, -0.43836134,  0.18767787], \\\n",
    "                         [-0.43836134, -1.72295801,  0.61817001], \\\n",
    "                         [ 0.18767787,  0.61817001, -0.51247901]])\n",
    "theta_test=np.array([-2,1,2])\n",
    "\n",
    "hess_l_test  = hess_l(theta_test,x,y)\n",
    "error_test=np.sum(np.abs(hess_l_test-target_value),axis=(0,1))\n",
    "\n",
    "print(\"{:f}\".format(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48hVqShUOeCG"
   },
   "source": [
    "Now run Newton's method to perform logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dhw9YOdVOeCH",
    "outputId": "f8bddc10-e9d3-4792-f8d4-73f1ba1d1e68"
   },
   "outputs": [],
   "source": [
    "# Initialize theta0\n",
    "theta0 = np.zeros(x.shape[1])\n",
    "\n",
    "# Run Newton's method\n",
    "theta_final, theta_history, log_l_history = newton(theta0,x,y,grad_l,hess_l,1e-6)\n",
    "print(theta_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swC4RWi0OeCJ"
   },
   "source": [
    "Let's plot the log likelihood over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5TDSytMMOeCK",
    "outputId": "06d5feca-7b74-4216-a5db-deb251c7f142"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(num=2)\n",
    "\n",
    "ax.set_ylabel('l(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(len(log_l_history)),log_l_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnGtGzYDOeCN"
   },
   "source": [
    "Plot the data and our decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "dhDCM-n4OeCO",
    "outputId": "383e7da5-82e7-4b61-df54-d4032a28b0eb"
   },
   "outputs": [],
   "source": [
    "df_x_pl=df_x.copy()\n",
    "df_y_pl=df_y.copy()\n",
    "df_x_pl.insert(0, \"y\", df_y)\n",
    "df_x_pl[\"y\"] = pd.to_numeric(df_x_pl[\"y\"],downcast='signed')\n",
    "df_x_pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "g_5r0nW5OeCR",
    "outputId": "8e8e832f-a6a4-426b-ad28-77607ba8adc3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate vector to plot decision boundary\n",
    "x1_vec = np.linspace(df_x_pl[\"x1\"].min(),df_x_pl[\"x1\"].max(),2);\n",
    "\n",
    "# Plot raw data\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"y\", data=df_x_pl);\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.plot(x1_vec,(-x1_vec*theta_final[1]-theta_final[0])/theta_final[2], color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27QoXQikOeCT"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rPY_EKzOeCT"
   },
   "source": [
    "Write now your considerations. Discuss in particular:\n",
    "- the different convergence speed of the Newton's method, compared to Gradient Ascent\n",
    "- the incurred cost for the different convergence speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po-FgiDPOeCT"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqj8HMJMOeCU"
   },
   "source": [
    "\n",
    "## [7 points] Question 3: Logistic Regression with non linear boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1phEkjHOeCU"
   },
   "source": [
    "### (a) Polynomial features for logistic regression\n",
    "\n",
    "Define new features, e.g. of 2nd and 3rd degrees, and learn a logistic regression classifier by using the new features, by using the Newton's optimization algorithm (or the gradient ascent one, if you like)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRbt5SbZPcn6"
   },
   "source": [
    "In particular, we would consider a polynomial boundary with equation:\n",
    "\n",
    "$f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2$\n",
    "\n",
    "We would therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n",
    "\n",
    "Create new arrays by stacking x (from the previous questions) and the new 7 features (in the order x1x1, x2x2, x1x2, x1x1x1, x2x2x2, x1x1x2, x1x2x2). In particular create x_new_quad by additionally stacking with x the quadratic features, and x_new_cubic by additionally stacking with x the quadratic and the cubic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQTbat6UOeCV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First extract features x1 and x2 from x and reshape them to x1 vector arrays\n",
    "x1 = x[:,1]\n",
    "x2 = x[:,2]\n",
    "x1 = x1.reshape(x1.shape[0], 1)\n",
    "x2 = x2.reshape(x2.shape[0], 1)\n",
    "print(x[:5,:]) # For visualization of the first 5 values\n",
    "print(x1[:5,:]) # For visualization of the first 5 values\n",
    "print(x2[:5,:]) # For visualization of the first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL2ZN00OOeCX"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsyCm-uNOeCY"
   },
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROZQgI2COeCY"
   },
   "outputs": [],
   "source": [
    "# Define features: x1x1, x2x2, x1x2, x1x1x1, x2x2x2, x1x1x2, x1x2x2\n",
    "\n",
    "# Stack the features together together with x, to define\n",
    "# x_new_quad [x, x1x1, x2x2, x1x2]\n",
    "# and x_new_cubic [x, x1x1, x2x2, x1x2, x1x1x1, x2x2x2, x1x1x2, x1x2x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KvgwVckOeCe"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if0-ozefOeCe"
   },
   "source": [
    "Now use the Newton's optimization algorithm to learn theta by maximizing the log-likelihood, both for the case of x_new_quad and x_new_cubic.\n",
    "\n",
    "Note: you may alternatively use gradient ascent, as in Question 1, if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVe936XNOeCe"
   },
   "outputs": [],
   "source": [
    "# Initialize theta0, in case of quadratic features\n",
    "theta0_quad = np.zeros(x_new_quad.shape[1])\n",
    "\n",
    "# Run Newton's method, in case of quadratic features\n",
    "theta_final_quad, theta_history_quad, log_l_history_quad = newton(theta0_quad,x_new_quad,y,grad_l,hess_l,1e-6)\n",
    "\n",
    "# Initialize theta0, in case of quadratic and cubic features\n",
    "theta0_cubic = np.zeros(x_new_cubic.shape[1])\n",
    "\n",
    "# Run Newton's method, in case of quadratic and cubic features\n",
    "theta_final_cubic, theta_history_cubic, log_l_history_cubic = newton(theta0_cubic,x_new_cubic,y,grad_l,hess_l,1e-6)\n",
    "\n",
    "# check and compare with previous results\n",
    "print(theta_final_quad)\n",
    "print(theta_final_cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "0fdHd2L8OeCh",
    "outputId": "90db5074-51ff-458e-be72-e5bd3ae4a935"
   },
   "outputs": [],
   "source": [
    "# Plot the log likelihood values in the optimization iterations, in one of the two cases.\n",
    "fig,ax = plt.subplots(num=2)\n",
    "\n",
    "ax.set_ylabel('l(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(len(log_l_history_cubic)),log_l_history_cubic,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyHrTkyVOeCj"
   },
   "source": [
    "### (b) Plot the computed non-linear boundary and discuss the questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLqrjKy9OeCj"
   },
   "source": [
    "First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRfx4RsEOeCj"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9z3OOOdOeCk"
   },
   "outputs": [],
   "source": [
    "def boundary_function(x1_vec, x2_vec, theta_final):\n",
    "    \n",
    "    x1_vec, x2_vec = np.meshgrid(x1_vec,x2_vec)\n",
    "    \n",
    "    if len(theta_final) == 6:\n",
    "        # boundary function value for features up to quadratic\n",
    "        c_0, c_1, c_2, c_3, c_4, c_5 = theta_final\n",
    "        # f =\n",
    "    elif len(theta_final) == 10:\n",
    "        # boundary function value for features up to cubic\n",
    "        c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9 = theta_final\n",
    "        # f = \n",
    "    else:\n",
    "        raise(\"Number of Parameters is not correct\")\n",
    "        \n",
    "    return x1_vec, x2_vec, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APohoz_HOeCq"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "y8dN6C_YOeCq",
    "outputId": "7537487b-2361-4de9-8024-51b67ba30436"
   },
   "outputs": [],
   "source": [
    "x1_vec = np.linspace(df_x_pl[\"x1\"].min()-1,df_x_pl[\"x1\"].max()+1,200);\n",
    "x2_vec = np.linspace(df_x_pl[\"x2\"].min()-1,df_x_pl[\"x2\"].max()+1,200);\n",
    "\n",
    "x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta_final_quad)\n",
    "\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"y\", data=df_x_pl);\n",
    "\n",
    "plt.contour(x1_vec, x2_vec, f, colors=\"red\", levels=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_vec = np.linspace(df_x_pl[\"x1\"].min()-1,df_x_pl[\"x1\"].max()+1,200);\n",
    "x2_vec = np.linspace(df_x_pl[\"x2\"].min()-1,df_x_pl[\"x2\"].max()+1,200);\n",
    "\n",
    "x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta_final_cubic)\n",
    "\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"y\", data=df_x_pl);\n",
    "\n",
    "plt.contour(x1_vec, x2_vec, f, colors=\"red\", levels=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2q06ghtctFD"
   },
   "source": [
    "################# Do not write above this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRKJBKIoOeCt"
   },
   "source": [
    "Write now your considerations. Discuss in particular:\n",
    "- What happens if you further increase the degree of the polynomial. How would the boundary change? Would you incur underfitting or overfitting?\n",
    "- Look at the boundary of the quadratic and cubic polynomial features. Do they meet your expectations? Why? Consider in particular the classifier decision boundary in areas where there are fewer training data, i.e. where the classifier is supposed to generalize. Hint: Try changing the minimum and maximum limits in vectors x1_vec and x2_vec before plotting the boundaries.\n",
    "- Look at the values of the original features, the 2nd and 3rd degree ones. Do you expect that normalizing the input features (either by setting their range to [-1,1] or by subtracting their mean and dividing by their standard deviation) would improve the classifier? Why yes or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqCIJlppcvpJ"
   },
   "source": [
    "################# Do not write below this line #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBXW0UPaOeCt"
   },
   "source": [
    "\n",
    "## [10 points, extra, not compulsory] Question 4: Gaussian Discriminant Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XCqvVbVOeCt"
   },
   "source": [
    "### (a) Review and implement classification with Gradient Discriminant Analysis (GDA)\n",
    "\n",
    "Recall the assumptions for GDA\n",
    "\n",
    "$\\begin{aligned} y & \\sim \\operatorname{Bernoulli}(\\phi) \\\\ x \\mid y=0 & \\sim \\mathcal{N}\\left(\\mu_{0}, \\Sigma\\right) \\\\ x \\mid y=1 & \\sim \\mathcal{N}\\left(\\mu_{1}, \\Sigma\\right) \\end{aligned}$\n",
    "\n",
    "Recall the probability distributions:\n",
    "\n",
    "$\\begin{aligned} p(y) &=\\phi^{y}(1-\\phi)^{1-y} \\\\ p(x \\mid y=0) &=\\frac{1}{(2 \\pi)^{d / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{0}\\right)^{T} \\Sigma^{-1}\\left(x-\\mu_{0}\\right)\\right) \\\\ p(x \\mid y=1) &=\\frac{1}{(2 \\pi)^{d / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{1}\\right)^{T} \\Sigma^{-1}\\left(x-\\mu_{1}\\right)\\right) \\end{aligned}$\n",
    "\n",
    "Overall, the parameters of the model are $\\phi, \\Sigma, \\mu_{0}$ and $\\mu_{1}$ and the log-likelihood is given by:\n",
    "\n",
    "$\\begin{aligned} \\ell\\left(\\phi, \\mu_{0}, \\mu_{1}, \\Sigma\\right) &=\\log \\prod_{i=1}^{n} p\\left(x^{(i)}, y^{(i)} ; \\phi, \\mu_{0}, \\mu_{1}, \\Sigma\\right) \\\\ &=\\log \\prod_{i=1}^{n} p\\left(x^{(i)} \\mid y^{(i)} ; \\mu_{0}, \\mu_{1}, \\Sigma\\right) p\\left(y^{(i)} ; \\phi\\right) \\end{aligned}$\n",
    "\n",
    "Parameters may be estimated by maximum likelihood estimate as follows:\n",
    "\n",
    "$\\begin{aligned} \\phi &=\\frac{1}{n} \\sum_{i=1}^{n} 1\\left\\{y^{(i)}=1\\right\\} \\\\ \\mu_{0} &=\\frac{\\sum_{i=1}^{n} 1\\left\\{y^{(i)}=0\\right\\} x^{(i)}}{\\sum_{i=1}^{n} 1\\left\\{y^{(i)}=0\\right\\}} \\\\ \\mu_{1} &=\\frac{\\sum_{i=1}^{n} 1\\left\\{y^{(i)}=1\\right\\} x^{(i)}}{\\sum_{i=1}^{n} 1\\left\\{y^{(i)}=1\\right\\}} \\\\ \\Sigma &=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x^{(i)}-\\mu_{y^{(i)}}\\right)\\left(x^{(i)}-\\mu_{y^{(i)}}\\right)^{T} \\end{aligned}$\n",
    "\n",
    "Consider the data variables as from the previous questions, x and y.\n",
    "For this question, we would select features $x^{(i)}_1$ and $x^{(i)}_2$ into the new varible x12, dropping the $x_0=1$ convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract the sole features x1 and x2 from x into an x12 array and reshape it to x2 vector arrays\n",
    "x12 = x[:,1:]\n",
    "print(x[:5,:]) # For visualization of the first 5 values\n",
    "print(x12[:5,:]) # For visualization of the first 5 values\n",
    "print(y[:5]) # For visualization of the first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now code up the equations above to learn the GDA parameters from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "################# Do not write above this line #################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phi(y):\n",
    "  # your code here\n",
    "  pass\n",
    "\n",
    "\n",
    "def compute_mu0(x, y):\n",
    "  # your code here\n",
    "  pass\n",
    "\n",
    "\n",
    "def compute_mu1(x, y):\n",
    "  # your code here\n",
    "  pass\n",
    "\n",
    "\n",
    "def compute_sigma(x, y, mu0, mu1):\n",
    "  # your code here\n",
    "  pass\n",
    "\n",
    "\n",
    "# computing p(x|y) for the Bayes rule\n",
    "def p_x_given_y(x,mu,sigma):\n",
    "  # your code here\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X0KeBHGEyMA"
   },
   "source": [
    "\n",
    "################# Do not write below this line #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support functions\n",
    "\n",
    "# Function p(y) for applying the Bayes rule\n",
    "def p_y(y,phi):\n",
    "    if y==1: return phi\n",
    "    else: return 1-phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "VJS9V7u-hr3C",
    "outputId": "736f893a-35af-4344-bba5-752903be8abe"
   },
   "outputs": [],
   "source": [
    "# Now estimate the GDA parameters\n",
    "phi = compute_phi(y)\n",
    "mu0 = compute_mu0(x12,y)\n",
    "mu1 = compute_mu1(x12,y)\n",
    "sigma = compute_sigma(x12, y, mu0, mu1)\n",
    "\n",
    "phi,mu0,mu1,sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2MEcgHI0zZG"
   },
   "source": [
    "Now that we know both the distributions of $p(y)$ and $p(x|y)$, we may estimate the posterior probability for an input $x$ via Bayes Rule:\n",
    "\n",
    "\\begin{equation}\n",
    "p(y \\mid x)=\\frac{p(x \\mid y) p(y)}{p(x)}\n",
    "\\end{equation}\n",
    "where $p(x)$ is the same for both classes and may be disregarded.\n",
    "\n",
    "This is used to classify each data point, by comparing $p(y=1|x) \\propto p(x \\mid y=1) p(y=1)$ and $p(y=0|x) \\propto p(x \\mid y=0) p(y=0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "QXOCMphE-s3W",
    "outputId": "fb86b8d4-0a70-410e-bb29-34f5ddc4e4d6"
   },
   "outputs": [],
   "source": [
    "selected_example=1\n",
    "print('Selected example =', x12[selected_example,:])\n",
    "\n",
    "# compute p(y=0|x) ~ p(x|y=0)*p(y=0)  &  p(y=1|x) ~ p(x|y=1)*p(y=1)\n",
    "\n",
    "# y=0\n",
    "print('p(y=0|x) ~', p_x_given_y(x12[selected_example,:],mu0,sigma)*p_y(0,phi))\n",
    "\n",
    "# y=1\n",
    "print('p(y=1|x) ~', p_x_given_y(x12[selected_example,:],mu1,sigma)*p_y(1,phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-LisANTOeCu"
   },
   "source": [
    "### (b) Plot the estimated likelihood functions, the decision boundary and write your remarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_gda(x1_vec, x2_vec, mu0, mu1, sigma, phi):\n",
    "    \n",
    "    x1_vec, x2_vec = np.meshgrid(x1_vec,x2_vec)\n",
    "    \n",
    "    f=np.zeros(x1_vec.shape)\n",
    "    \n",
    "    for i in range(f.shape[0]):\n",
    "        for j in range(f.shape[1]):\n",
    "            x12_ij = np.array( (x1_vec[i,j],x2_vec[i,j]) )\n",
    "            f[i,j] = p_x_given_y(x12_ij,mu1,sigma)*p_y(1,phi) - p_x_given_y(x12_ij,mu0,sigma)*p_y(0,phi)\n",
    "            \n",
    "    return x1_vec, x2_vec, f\n",
    "\n",
    "\n",
    "x1_vec = np.linspace(df_x_pl[\"x1\"].min()-1,df_x_pl[\"x1\"].max()+1,200);\n",
    "x2_vec = np.linspace(df_x_pl[\"x2\"].min()-1,df_x_pl[\"x2\"].max()+1,200);\n",
    "\n",
    "x1_vec, x2_vec, f = boundary_gda(x1_vec, x2_vec, mu0, mu1, sigma, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Define Gaussian pdfs\n",
    "rv0 = multivariate_normal(mu0, sigma)\n",
    "rv1 = multivariate_normal(mu1, sigma)\n",
    "\n",
    "pairx12 = np.dstack((x1_vec, x2_vec))\n",
    "\n",
    "# plot pdf contours\n",
    "plt.contourf(x1_vec, x2_vec, rv0.pdf(pairx12),alpha=0.5,cmap=cm.Reds)\n",
    "plt.contourf(x1_vec, x2_vec, rv1.pdf(pairx12),alpha=0.35,cmap=cm.Blues)\n",
    "\n",
    "# Add the GDA classification to the plot\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"y\", data=df_x_pl);\n",
    "plt.contour(x1_vec, x2_vec, f, colors=\"red\", levels=[0])\n",
    "\n",
    "plt.ylim(df_x_pl[\"x2\"].min()-1,df_x_pl[\"x2\"].max()+1)\n",
    "plt.xlim(df_x_pl[\"x1\"].min()-1,df_x_pl[\"x1\"].max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison with the Logistic Regression boundary\n",
    "\n",
    "# Generate vector to plot decision boundary\n",
    "x1vec = np.linspace(df_x_pl[\"x1\"].min()-1,df_x_pl[\"x1\"].max()+1,2)\n",
    "\n",
    "# Plot raw data\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", hue=\"y\", data=df_x_pl)\n",
    "\n",
    "# Plot decision boundary\n",
    "log, = plt.plot(x1vec,(-x1vec*theta_GA[1]-theta_GA[0])/theta_GA[2], color=\"red\")\n",
    "gda = plt.contour(x1_vec, x2_vec, f, colors=\"green\", levels=[0])\n",
    "\n",
    "plt.legend([log, gda.collections[0]],['logistic regression','GDA'])\n",
    "\n",
    "plt.ylim(df_x_pl[\"x2\"].min()-1,df_x_pl[\"x2\"].max()+1)\n",
    "plt.xlim(df_x_pl[\"x1\"].min()-1,df_x_pl[\"x1\"].max()+1)\n",
    "\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiSJyf3lOeCu"
   },
   "source": [
    "Write now your considerations. Discuss in particular:\n",
    "- Gaussian Discriminant Analysis makes stronger modelling assumptions than logistic regression. When may this be helpful and where may this be harmful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6KSzi2BOeCu"
   },
   "source": [
    "Credits for material: Andrew Ng, C. Combier"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7XCqvVbVOeCt",
    "j-LisANTOeCu",
    "Z2o9jg7EOeCv"
   ],
   "name": "FDS_Exercise_2_own.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
